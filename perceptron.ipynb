{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "#from mltools.dataManager import DataManager\n",
    "from mltools.baseLearner import BaseLearner\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    MISSING = float(\"infinity\")\n",
    "\n",
    "    def __init__(self, arff=None):\n",
    "        \"\"\"\n",
    "        If matrix is provided, all parameters must be provided, and the new matrix will be\n",
    "        initialized with the specified portion of the provided matrix.\n",
    "        \"\"\"\n",
    "        if arff:\n",
    "            self.load_arff(arff)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def init_from(self, matrix, row_start, col_start, row_count, col_count):\n",
    "        \"\"\"Initialize the matrix with a portion of another matrix\"\"\"\n",
    "        self.data = [matrix.data[row][col_start:col_start+col_count] for row in range(row_start, row_start+row_count)]\n",
    "        self.attr_names = matrix.attr_names[col_start:col_start+col_count]\n",
    "        self.str_to_enum = matrix.str_to_enum[col_start:col_start+col_count]    # array of dictionaries\n",
    "        self.enum_to_str = matrix.enum_to_str[col_start:col_start+col_count]    # array of dictionaries\n",
    "        return self\n",
    "\n",
    "    def add_data(self, new_data):\n",
    "        \"\"\"Appends a copy of the specified portion of a matrix to this matrix\"\"\"\n",
    "        self.data = np.vstack((self.data, new_data))\n",
    "\n",
    "    def set_labels(self):\n",
    "        self.labels = self.data[:,-1]\n",
    "        self.data = self.data[:,:-1]\n",
    "\n",
    "    def test_train_split(self, ratio=.8):\n",
    "        self.shuffle()\n",
    "        split_point = int(self.num_rows * ratio)\n",
    "        x_train = self.data[:split_point,:]\n",
    "        x_test = self.data[split_point:,:]\n",
    "        y_train = self.labels[:split_point]\n",
    "        y_test = self.labels[split_point:]\n",
    "        return x_train, x_test, y_train, y_test\n",
    "\n",
    "    def set_size(self, rows, cols):\n",
    "        \"\"\"Resize this matrix (and set all attributes to be continuous)\"\"\"\n",
    "        self.data = [[0]*cols for row in range(rows)]\n",
    "        self.attr_names = [\"\"] * cols\n",
    "        self.str_to_enum = {}\n",
    "        self.enum_to_str = {}\n",
    "\n",
    "    def load_arff(self, filename):\n",
    "        \"\"\" Load data from an ARFF file \"\"\"\n",
    "        self.data = []\n",
    "        self.attr_names = []\n",
    "        self.str_to_enum = []\n",
    "        self.enum_to_str = []\n",
    "        reading_data = False\n",
    "\n",
    "        rows = []           # we read data into array of rows, then convert into array of columns\n",
    "\n",
    "        f = open(filename)\n",
    "        for line in f.readlines():\n",
    "            line = line.rstrip().upper()\n",
    "            if len(line) > 0 and line[0] != '%':\n",
    "                if not reading_data:\n",
    "                    if line.startswith(\"@RELATION\"):\n",
    "                        self.dataset_name = line[9:].strip()\n",
    "                    elif line.startswith(\"@ATTRIBUTE\"):\n",
    "                        attr_def = line[10:].strip()\n",
    "                        if attr_def[0] == \"'\":\n",
    "                            attr_def = attr_def[1:]\n",
    "                            attr_name = attr_def[:attr_def.index(\"'\")]\n",
    "                            attr_def = attr_def[attr_def.index(\"'\")+1:].strip()\n",
    "                        else:\n",
    "                            attr_name, attr_def = attr_def.split()\n",
    "\n",
    "                        self.attr_names += [attr_name]\n",
    "\n",
    "                        str_to_enum = {}\n",
    "                        enum_to_str = {}\n",
    "                        if not(attr_def == \"REAL\" or attr_def == \"CONTINUOUS\" or attr_def == \"INTEGER\"):\n",
    "                            # attribute is discrete\n",
    "                            assert attr_def[0] == '{' and attr_def[-1] == '}'\n",
    "                            attr_def = attr_def[1:-1]\n",
    "                            attr_vals = attr_def.split(\",\")\n",
    "                            val_idx = 0\n",
    "                            for val in attr_vals:\n",
    "                                val = val.strip()\n",
    "                                enum_to_str[val_idx] = val\n",
    "                                str_to_enum[val] = val_idx\n",
    "                                val_idx += 1\n",
    "\n",
    "                        self.enum_to_str.append(enum_to_str)\n",
    "                        self.str_to_enum.append(str_to_enum)\n",
    "\n",
    "                    elif line.startswith(\"@DATA\"):\n",
    "                        reading_data = True\n",
    "\n",
    "                else:\n",
    "                    # reading data\n",
    "                    row = []\n",
    "                    val_idx = 0\n",
    "                    # print(\"{}\".format(line))\n",
    "                    vals = line.split(\",\")\n",
    "                    for val in vals:\n",
    "                        val = val.strip()\n",
    "                        if not val:\n",
    "                            raise Exception(\"Missing data element in row with data '{}'\".format(line))\n",
    "                        else:\n",
    "                            row += [float(self.MISSING if val == \"?\" else self.str_to_enum[val_idx].get(val, val))]\n",
    "\n",
    "                        val_idx += 1\n",
    "\n",
    "                    rows += [row]\n",
    "\n",
    "        f.close()\n",
    "        self.data=np.array(rows)\n",
    "        self.set_labels()\n",
    "\n",
    "\n",
    "    @property\n",
    "    def num_rows(self):\n",
    "        \"\"\"Get the number of rows in the matrix\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    @property\n",
    "    def num_cols(self):\n",
    "        \"\"\"Get the number of columns (or attributes) in the matrix\"\"\"\n",
    "        return len(self.attr_names)\n",
    "\n",
    "    \n",
    "    def attr_name(self, col):\n",
    "        \"\"\"Get the name of the specified attribute\"\"\"\n",
    "        return self.attr_names[col]\n",
    "\n",
    "    def attr_value(self, attr, val):\n",
    "        \"\"\"\n",
    "        Get the name of the specified value (attr is a column index)\n",
    "        :param attr: index of the column\n",
    "        :param val: index of the value in the column attribute list\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.enum_to_str[attr][val]\n",
    "\n",
    "    def value_count(self, col):\n",
    "        \"\"\"\n",
    "        Get the number of values associated with the specified attribute (or columnn)\n",
    "        0=continuous, 2=binary, 3=trinary, etc.\n",
    "        \"\"\"\n",
    "        return len(self.enum_to_str[col]) if len(self.enum_to_str) > 0 else 0\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"Shuffle the row order. If a buddy Matrix is provided, it will be shuffled in the same order.\"\"\"\n",
    "        idx = np.random.permutation(self.num_rows)\n",
    "        self.data = self.data[idx]\n",
    "        self.labels = self.labels[idx]\n",
    "\n",
    "     \n",
    "    def mean(self):\n",
    "        return np.mean(self.data, axis=0)\n",
    "\n",
    "    def column_mean(self, col):\n",
    "        \"\"\"Get the mean of the specified column\"\"\"\n",
    "        a = np.ma.masked_equal(self.data[:,col], self.MISSING).compressed()\n",
    "        return np.mean(a)\n",
    "\n",
    "    def column_min(self, col):\n",
    "        \"\"\"Get the min value in the specified column\"\"\"\n",
    "        a = np.ma.masked_equal(self.data[:,col], self.MISSING).compressed()\n",
    "        return np.min(a)\n",
    "\n",
    "    def column_max(self, col):\n",
    "        \"\"\"Get the max value in the specified column\"\"\"\n",
    "        a = np.ma.masked_equal(self.data[:,col], self.MISSING).compressed()\n",
    "        return np.max(a)\n",
    "\n",
    "    def most_common_value(self, col):\n",
    "        \"\"\"Get the most common value in the specified column\"\"\"\n",
    "        a = np.ma.masked_equal(self.data[:,col], self.MISSING).compressed()\n",
    "        scores = np.unique(np.ravel(a))       # get ALL unique values\n",
    "        testshape = list(a.shape)\n",
    "        testshape[axis] = 1\n",
    "        oldmostfreq = np.zeros(testshape)\n",
    "        oldcounts = np.zeros(testshape)\n",
    "\n",
    "        for score in scores:\n",
    "            template = (a == score)\n",
    "            counts = np.expand_dims(np.sum(template, axis),axis)\n",
    "            mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)\n",
    "            oldcounts = np.maximum(counts, oldcounts)\n",
    "            oldmostfreq = mostfrequent\n",
    "        \n",
    "        return mostfrequent\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Normalize each column of continuous values\"\"\"\n",
    "        for i in range(self.num_cols):\n",
    "            if self.value_count(i) == 0:     # is continuous\n",
    "                min_val = self.column_min(i)\n",
    "                max_val = self.column_max(i)\n",
    "                for j in range(self.num_rows):\n",
    "                    v = self.data[j, i]\n",
    "                    if v != self.MISSING:\n",
    "                        self.data[j, i] = (v - min_val)/(max_val - min_val)\n",
    "                        \n",
    "    def __str__(self):\n",
    "        outString = \"\"\n",
    "        print(\"@RELATION {}\".format(self.dataset_name))\n",
    "        for i in range(len(self.attr_names)):\n",
    "            print(\"@ATTRIBUTE {}\".format(self.attr_names[i]), end=\"\")\n",
    "            if self.value_count(i) == 0:\n",
    "                print(\" CONTINUOUS\")\n",
    "            else:\n",
    "                print(\" {{{}}}\".format(\", \".join(self.enum_to_str[i].values())))\n",
    "\n",
    "        print(\"@DATA\")\n",
    "        print(self.data)\n",
    "\n",
    "    def __str__(self):\n",
    "        outString = \"\"\n",
    "        outString += \"@RELATION {}\".format(self.dataset_name)\n",
    "        for i in range(len(self.attr_names)):\n",
    "            outString += \"@ATTRIBUTE {}\\n\".format(self.attr_names[i])\n",
    "            if self.value_count(i) == 0:\n",
    "                outString += \" CONTINUOUS\\n\"\n",
    "            else:\n",
    "                outString += \" {{{}}}\\n\".format(\", \".join(self.enum_to_str[i].values()))\n",
    "\n",
    "        outString += \"@DATA\\n\"\n",
    "        outString += str(self.data)\n",
    "        return outString\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@RELATION SEPERABLE@ATTRIBUTE X\n",
      " CONTINUOUS\n",
      "@ATTRIBUTE Y\n",
      " CONTINUOUS\n",
      "@ATTRIBUTE CLASS\n",
      " {1, 0}\n",
      "@DATA\n",
      "[[ 0.25  0.25]\n",
      " [ 0.1   0.1 ]\n",
      " [ 0.5   0.25]\n",
      " [ 0.2   0.75]\n",
      " [-0.25 -0.25]\n",
      " [-0.1  -0.1 ]\n",
      " [-0.5  -0.25]\n",
      " [-0.2  -0.75]]\n"
     ]
    }
   ],
   "source": [
    "d = DataManager(arff='./datasets/seperable.arff')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = DataManager(arff='./mltools/data/voting.arff')\n",
    "x_train, x_test, y_train, y_test = data.test_train_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Perceptron(BaseLearner):\n",
    "    \n",
    "    def train(self, x_train, y_train, lr=.001, thresh=0):\n",
    "        # Add Bias\n",
    "        x_train = np.hstack((np.ones((x_train.shape[0],1)), x_train))\n",
    "        n, d = x_train.shape\n",
    "        \n",
    "        # Initialize Weights\n",
    "        self.w = np.random.randn(d)\n",
    "        \n",
    "        for j in range(1,20):\n",
    "            for i in range(n):\n",
    "                predicted = np.dot(self.w, x_train[i])\n",
    "                predicted = 0 if predicted < thresh else 1\n",
    "                self.w = self.w - (lr * (y_train[i] - predicted) * x_train[i])\n",
    "            \n",
    "    \n",
    "    def predict(self, x_test, thresh=0):\n",
    "        x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n",
    "        probs = np.dot(x_test, self.w)\n",
    "        probs[probs > thresh] = 1\n",
    "        probs[probs <= thresh] = 0\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Perceptron()\n",
    "model.train(x_train, y_train)\n",
    "y_hat = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39673913043478259"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_hat, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  (40%) Correctly implement the perceptron learning algorithm.  For this and the following projects, you should integrate the learning algorithm with the tool kit or with your variation of the tool kit if you do your own.  Before implementing your perceptron and other projects, you should review requirements for the project so that you implement it in a way that will support them all. Note that for this and the other labs it is not always easy to tell if you have implemented the model exactly correct, since training sets, initial parameters, etc. usually have a random aspect. However, it is easy to see when your results are inconsistent with reasonable results, and points will be reduced based on how far off your implementation appears to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 (5%) Create 2 ARFF files, both with 8 instances using 2 real valued inputs (which range between -1 and 1) each with 4 instances from each class.  One should be linearly separable and the other not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAECCAYAAAAW+Nd4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1187a3190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = DataManager('datasets/nonseperable.arff')\n",
    "plt.scatter(test.data[:,0], test.data[:,1], c=test.labels, cmap=cm.Accent)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  (10%) Train on both sets (the entire sets) with the Perceptron Rule. Try it with a couple different learning rates and discuss the effect of learning rate, including how many epochs are completed before stopping.  (For these situations learning rate should have minimal effect, unlike with the Backpropagation lab).  The basic stopping criteria for many models is to stop training when no longer making significant progress.  Most commonly, when you have gone a number of epochs (e.g. 5) with no significant improvement in accuracy (Note that the weights/accuracy do not usually change monotonically).  Describe your specific stopping criteria.  Don’t just stop the first epoch when no improvement occurs.  Use a learning rate of .1 for experiments 4-6 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (10%) Graph the instances and decision line for the two cases above (with LR=.1). For all graphs always label the axes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (20%) Use the perceptron rule to learn this version of the voting task.  This particular task is an edited version of the standard voting set, where we have replaced all the “don’t know” values with the most common value for the particular attribute.  Randomly split the data into 70% training and 30% test set.  Try it five times with different random 70/30 splits.  For each split report the final training and test set accuracy and the # of epochs required.  Also report the average of these values from the 5 trials.  You should update after every instance.  Remember to shuffle the data order after each epoch.  By looking at the weights, explain what the model has learned and how the individual input features affect the result.  Which specific features are most critical for the voting task, and which are least critical?  Do one graph of the average misclassification rate vs epochs (0th – final epoch) for the training set. In our helps page is some help for doing graphs.  As a rough sanity check, typical Perceptron accuracies for the voting data set are 90%-98%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.  (15%) Do your own experiment with either the perceptron or delta rule.  Include in your discussion what you learned from the experiment.  Have fun and be creative!  For this lab and all the future labs make sure you do something more than just try out the model on different data sets.  One option you can do to fulfill this part is the following:\n",
    "\n",
    "Use the perceptron rule to learn the iris task or some other task with more than two possible output values. Note that the iris data set has 3 output classes, and a perceptron node only has two possible outputs.  Two common ways to deal with this are:\n",
    "\n",
    "a)  Create 1 perceptron for each output class.  Each perceptron has its own training set which considers its class positive and all other classes to be negative examples.  Run all three perceptrons on novel data and set the class to the label of the perceptron which outputs high.  If there is a tie, choose the perceptron with the highest net value.\n",
    "\n",
    "b)  Create 1 perceptron for each pair of output classes, where the training set only contains examples from the 2 classes.  Run all perceptrons on novel data and set the class to the label with the most wins (votes) from the perceptrons.  In case of a tie, use the net values to decide.\n",
    "\n",
    "You could implement one of these.  For either of these approaches you can train up the models independently or simultaneously.  For testing you just execute the novel instance on each model and combine the overall results to see which output class wins.\n",
    "\n",
    "Note:  In order to help you debug this and other projects we have included some small examples and other hints with actual learned hypotheses so that you can compare the results of your code and help ensure that your code is working properly.  You may also discuss and compare results with classmates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
