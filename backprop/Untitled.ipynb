{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from queue import Queue\n",
    "from mltools.dataManager import DataManager\n",
    "from mltools.models import baseLearner\n",
    "from mltools.nnet.layers import *\n",
    "from mltools.nnet.optim import adam, sgd\n",
    "from mltools.utils import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    \n",
    "    def __init__(self, batch_size=32, hidden_dim=100, epochs=100, learning_rate=.001, reg=0, early_stop=10):\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.HIDDEN_DIM = hidden_dim\n",
    "        self.EPOCHS = epochs\n",
    "        self.REG = reg\n",
    "        self.LR = learning_rate\n",
    "        self.NUM_CLASSES = None\n",
    "        self.EARLY_STOP = early_stop\n",
    "    \n",
    "    \n",
    "    def init_weights(self, verbose=True):\n",
    "        # Initialize weights to train\n",
    "        params = {}\n",
    "        params['W1'] = np.random.normal(scale=.1, size=(self.INPUT_DIM, self.HIDDEN_DIM))\n",
    "        params['W2'] = np.random.normal(scale=.1, size=(self.HIDDEN_DIM, self.NUM_CLASSES))\n",
    "        #params['W1'] = np.ones((self.INPUT_DIM, self.HIDDEN_DIM))\n",
    "        #params['W2'] = np.ones((self.HIDDEN_DIM, self.NUM_CLASSES))\n",
    "        params['b1'] = np.ones(self.HIDDEN_DIM)\n",
    "        params['b2'] = np.ones(self.NUM_CLASSES)\n",
    "        self.params = params\n",
    "        if verbose:\n",
    "            print('\\nInitialized Paramaters:')\n",
    "            for name, tensor in params.iteritems():\n",
    "                print('%s: ' % name, tensor.shape)\n",
    "                \n",
    "    def train(self, x_train, y_train, optim='sgd', num_classes=None, verbose=True):\n",
    "        \n",
    "        if num_classes is None:\n",
    "            num_classes = int(np.max(y_train) + 1)\n",
    "            self.NUM_CLASSES = num_classes\n",
    "        temp = DataManager()\n",
    "        temp.set_data(x_train, y_train)\n",
    "        x_train, x_val, y_train_num, y_val_num = temp.test_train_split(.9)\n",
    "        y_train = one_hot(y_train_num, num_class=num_classes)\n",
    "        y_val = one_hot(y_val_num, num_class=num_classes)\n",
    "        \n",
    "            \n",
    "        self.INPUT_DIM = x_train.shape[1]\n",
    "        self.init_weights(False)\n",
    "        params = self.params\n",
    "        \n",
    "        # Trianing\n",
    "        num_training = x_train.shape[0]\n",
    "        iters_per_epoch = int(max(num_training / self.BATCH_SIZE, 1))\n",
    "        num_iterations = int(self.EPOCHS * iters_per_epoch)\n",
    "\n",
    "        # For logging results\n",
    "        self.loss_hist_train = []\n",
    "        self.loss_hist_val = []\n",
    "        self.acc_hist_train = []\n",
    "        self.acc_hist_val = []\n",
    "        self.num_hist_train = []\n",
    "        self.num_hist_val = []\n",
    "        \n",
    "        if verbose: print('\\nStarting Training:')\n",
    "        for epoch in range(self.EPOCHS):\n",
    "            # Shuffle the data\n",
    "            shuffle_idx = np.random.permutation(range(x_train.shape[0]))\n",
    "            x_train = x_train[shuffle_idx]\n",
    "            y_train = y_train[shuffle_idx]\n",
    "            y_train_num = y_train_num[shuffle_idx]\n",
    "\n",
    "            for i in range(iters_per_epoch):\n",
    "                # Make minibatch\n",
    "                batch_start = self.BATCH_SIZE*i\n",
    "                batch_end = self.BATCH_SIZE*(i+1)\n",
    "                x_batch = x_train[batch_start:batch_end]\n",
    "                y_batch = y_train[batch_start:batch_end]\n",
    "                y_batch_num = y_train_num[batch_start:batch_end]\n",
    "\n",
    "                # Forward Pass\n",
    "                h1, linear_cache_1 = linear_forward(x_batch, params['W1'], params['b1'])\n",
    "                a1, relu_cache_1= relu_forward(h1)\n",
    "                h2, linear_cache_2 = linear_forward(a1, params['W2'], params['b2'])\n",
    "                a2, relu_cache_2 = relu_forward(h2)\n",
    "\n",
    "                # Loss\n",
    "                probs, loss, dx = softmax_loss(a2, y_batch_num)\n",
    "                # Plus regularization\n",
    "                loss += .5 * self.REG * np.sum(params['W1']**2) + .5 * self.REG * np.sum(params['W2']**2)\n",
    "                self.loss_hist_train.append(loss)\n",
    "\n",
    "                # Backwards Pass\n",
    "                grads = {}\n",
    "                da = relu_backward(dx, relu_cache_2)\n",
    "                dx_2, grads['W2'], grads['b2'] = linear_backward(da, linear_cache_2)\n",
    "                da_2 = relu_backward(dx_2, relu_cache_1)\n",
    "                dx, grads['W1'], grads['b1'] = linear_backward(da_2, linear_cache_1)\n",
    "\n",
    "                # Regularization (optional)\n",
    "                grads['W2'] += self.REG * params['W2']\n",
    "                grads['W1'] += self.REG * params['W1']\n",
    "\n",
    "                # Parameter update\n",
    "                for p, w in params.iteritems():\n",
    "                    dw = grads[p]\n",
    "                    if optim == 'sgd':\n",
    "                        next_w, next_config = sgd(w, dw, config={'learning_rate':self.LR})\n",
    "                    else:\n",
    "                        # Optimization using momentum\n",
    "                        next_w, next_config = adam(w, dw, config={'learning_rate':self.LR})\n",
    "                    # Update weights\n",
    "                    params[p] = next_w\n",
    "    \n",
    "            # Calculate the accuracy\n",
    "            probs = np.exp(a2 - np.max(a2, axis=1, keepdims=True))\n",
    "            probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "            y_pred = np.argmax(probs, axis=1)\n",
    "            train_acc = np.mean(y_pred == y_batch_num)\n",
    "            num_correct = np.sum(y_pred == y_batch_num)\n",
    "            self.acc_hist_train.append(train_acc)\n",
    "            self.num_hist_train.append(num_correct)\n",
    "            if verbose: print(\"[{}] loss: {}, Acc: {}\".format(epoch, loss, train_acc))\n",
    "                \n",
    "            # calc accuaracy on validation\n",
    "            probs = self.predict(x_val, hard=False)\n",
    "            #mse = mean_squared_error(probs, y_val)\n",
    "            y_pred = np.argmax(probs, axis=1)\n",
    "            train_acc = np.mean(y_pred == y_val_num)\n",
    "            num_correct = np.sum(y_pred == y_val_num)\n",
    "            self.acc_hist_val.append(train_acc)\n",
    "            self.num_hist_val.append(num_correct)\n",
    "            \n",
    "            # Early stopping\n",
    "            past_acc = self.acc_hist_val[-5:]\n",
    "            if np.array_equal(past_acc[::-1], np.sort(past_acc)):\n",
    "                print(\"STOP\")\n",
    "                print(past_acc)\n",
    "\n",
    "            \n",
    "        self.params = params  \n",
    "        if verbose: print('\\n---Completed Training---')\n",
    "            \n",
    "    def predict(self, x_test,hard=True):\n",
    "        params = self.params\n",
    "        h1, _ = linear_forward(x_test, params['W1'], params['b1'])\n",
    "        a1, _ = relu_forward(h1)\n",
    "        h2, _ = linear_forward(a1, params['W2'], params['b2'])\n",
    "        a2, _ = relu_forward(h2)\n",
    "        probs = np.exp(a2 - np.max(a2, axis=1, keepdims=True))\n",
    "        probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "        if hard:\n",
    "            return np.argmax(probs, axis=1)\n",
    "        return probs\n",
    "            \n",
    "    def accuracy(self, x_test, y_test):\n",
    "        probs = self.predict(x_test)\n",
    "        y_pred = np.argmax(probs, axis=1)\n",
    "        return np.mean(y_pred == y_test)\n",
    "        \n",
    "    def mse(self, x_test, y_test):\n",
    "        probs = self.predict(x_test)\n",
    "        return mean_squared_error(probs, y_test)       \n",
    "    \n",
    "    def save_model(self, output):\n",
    "        pickle.dump(self.params, open(output, 'wb'))\n",
    "        \n",
    "    def load_model(in_file):\n",
    "        self.params = pickle.load(open(in_file, 'rb'))\n",
    "        self.HIDDEN_DIM = self.params['W1'].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = DataManager('datasets/iris.arff')\n",
    "data.normalize()\n",
    "x_train, x_test, y_train_num, y_test_num = data.test_train_split(.75)\n",
    "y_train = one_hot(y_train_num, num_class=3)\n",
    "y_test = one_hot(y_test_num, num_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training:\n",
      "[0] loss: 1.10923097281, Acc: 0.38\n",
      "STOP\n",
      "[0.25]\n",
      "[1] loss: 1.09717144106, Acc: 0.5\n",
      "STOP\n",
      "[0.25, 0.25]\n",
      "[2] loss: 1.10083028487, Acc: 0.3\n",
      "STOP\n",
      "[0.25, 0.25, 0.25]\n",
      "[3] loss: 1.09090181523, Acc: 0.3\n",
      "STOP\n",
      "[0.25, 0.25, 0.25, 0.25]\n",
      "[4] loss: 1.03022420512, Acc: 0.42\n",
      "STOP\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "[5] loss: 1.01607414939, Acc: 0.4\n",
      "STOP\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "[6] loss: 0.999512343641, Acc: 0.38\n",
      "STOP\n",
      "[0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "[7] loss: 1.01328402281, Acc: 0.28\n",
      "[8] loss: 0.894164610704, Acc: 0.64\n",
      "[9] loss: 0.860351546525, Acc: 0.54\n",
      "[10] loss: 0.75958962933, Acc: 0.76\n",
      "[11] loss: 0.794779238343, Acc: 0.56\n",
      "STOP\n",
      "[0.41666666666666669, 0.41666666666666669, 0.41666666666666669, 0.41666666666666669, 0.41666666666666669]\n",
      "[12] loss: 0.706168233716, Acc: 0.62\n",
      "[13] loss: 0.659664803787, Acc: 0.66\n",
      "[14] loss: 0.637467119484, Acc: 0.86\n",
      "[15] loss: 0.621111697625, Acc: 0.9\n",
      "[16] loss: 0.560051835415, Acc: 0.96\n",
      "[17] loss: 0.642498813696, Acc: 0.92\n",
      "[18] loss: 0.559049138993, Acc: 0.86\n",
      "[19] loss: 0.534476232688, Acc: 0.9\n",
      "STOP\n",
      "[1.0, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[20] loss: 0.488409761325, Acc: 0.96\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[21] loss: 0.498628894208, Acc: 0.9\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[22] loss: 0.46478745364, Acc: 0.86\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[23] loss: 0.493644151577, Acc: 1.0\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[24] loss: 0.436950789512, Acc: 0.92\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[25] loss: 0.41995506133, Acc: 0.96\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[26] loss: 0.381247266085, Acc: 0.96\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[27] loss: 0.385515467298, Acc: 0.94\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[28] loss: 0.410871645356, Acc: 0.92\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[29] loss: 0.411926533261, Acc: 0.94\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[30] loss: 0.408513529865, Acc: 0.94\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[31] loss: 0.42985640172, Acc: 0.92\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[32] loss: 0.434476816323, Acc: 0.92\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[33] loss: 0.33701560195, Acc: 0.94\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[34] loss: 0.339834435956, Acc: 0.94\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[35] loss: 0.339793687901, Acc: 0.96\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[36] loss: 0.328272688224, Acc: 0.92\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[37] loss: 0.315813820717, Acc: 0.94\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[38] loss: 0.307358385931, Acc: 0.96\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[39] loss: 0.29651641725, Acc: 0.98\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[40] loss: 0.287333970813, Acc: 1.0\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[41] loss: 0.292980962811, Acc: 0.96\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[42] loss: 0.3062462034, Acc: 0.9\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[43] loss: 0.283920493049, Acc: 0.94\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[44] loss: 0.272940927034, Acc: 0.96\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[45] loss: 0.293726643362, Acc: 0.96\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[46] loss: 0.263503131825, Acc: 0.98\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[47] loss: 0.282370099832, Acc: 0.94\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[48] loss: 0.257863081225, Acc: 0.94\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[49] loss: 0.265959955074, Acc: 0.96\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[50] loss: 0.269178991321, Acc: 0.92\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[51] loss: 0.262872694425, Acc: 0.96\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[52] loss: 0.309453243675, Acc: 0.92\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[53] loss: 0.233943032479, Acc: 0.94\n",
      "STOP\n",
      "[0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663, 0.91666666666666663]\n",
      "[54] loss: 0.258875092263, Acc: 0.94\n",
      "[55] loss: 0.210013991928, Acc: 0.94\n",
      "[56] loss: 0.248919557683, Acc: 0.94\n",
      "[57] loss: 0.205123229336, Acc: 0.96\n",
      "[58] loss: 0.243985336874, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[59] loss: 0.283418264005, Acc: 0.9\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[60] loss: 0.211283970923, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[61] loss: 0.194271105465, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[62] loss: 0.226471971256, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[63] loss: 0.229405482457, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[64] loss: 0.258730973762, Acc: 0.92\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[65] loss: 0.222289457946, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[66] loss: 0.213913554883, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[67] loss: 0.186250388429, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[68] loss: 0.182657159965, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[69] loss: 0.208263160461, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[70] loss: 0.267125997816, Acc: 0.92\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[71] loss: 0.205268107191, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[72] loss: 0.171911219722, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[73] loss: 0.13229588928, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[74] loss: 0.162278441264, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[75] loss: 0.200580333271, Acc: 0.92\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[76] loss: 0.195719824071, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[77] loss: 0.192759844566, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[78] loss: 0.177308737668, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[79] loss: 0.166634596691, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[80] loss: 0.179189407758, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[81] loss: 0.194811950847, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[82] loss: 0.169727158559, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[83] loss: 0.132387029921, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[84] loss: 0.162992286616, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[85] loss: 0.171126240457, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[86] loss: 0.137906048248, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[87] loss: 0.18294198003, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[88] loss: 0.16523225155, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[89] loss: 0.145943630007, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[90] loss: 0.159887119827, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[91] loss: 0.186358642944, Acc: 0.92\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[92] loss: 0.146841008913, Acc: 0.96\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[93] loss: 0.126385367619, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[94] loss: 0.130882150927, Acc: 0.98\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[95] loss: 0.174526012147, Acc: 0.92\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[96] loss: 0.157973100906, Acc: 0.92\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[97] loss: 0.142413185136, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[98] loss: 0.152135854552, Acc: 0.94\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[99] loss: 0.163696497666, Acc: 0.92\n",
      "STOP\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "---Completed Training---\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNet(learning_rate=.1, hidden_dim=8, batch_size=50)\n",
    "net.train(x_train, y_train_num, optim='sgd', verbose=True)\n",
    "y_pred = net.predict(x_test, hard=True)\n",
    "print(np.mean(y_pred == y_test_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.predict(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = DataManager('datasets/vowel.arff')\n",
    "data.normalize()\n",
    "x_train, x_test, y_train_num, y_test_num = data.test_train_split(.75)\n",
    "y_train = one_hot(y_train_num, num_class=11)\n",
    "y_test = one_hot(y_test_num, num_class=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(learning_rate=.01)\n",
    "net.train(x_train, y_train_num, optim='sgd', verbose=False)\n",
    "y_pred = net.predict(x_test)\n",
    "test_acc = np.mean(y_pred == y_test_num)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_results = []\n",
    "for lr in [.001, .01, .1, 1, 10]:\n",
    "    acc_list = []\n",
    "    for i in range(5):\n",
    "        net = NeuralNet(learning_rate=lr)\n",
    "        net.train(x_train, y_train_num, optim='sgd', verbose=False)\n",
    "        y_pred = net.predict(x_test)\n",
    "        test_acc = np.mean(y_pred == y_test_num)\n",
    "        acc_list.append(test_acc)\n",
    "    avg = np.mean(acc_list)\n",
    "    lr_results.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(lr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = DataManager('datasets/vowel.arff')\n",
    "data.normalize()\n",
    "x_train, x_test, y_train_num, y_test_num = data.test_train_split(.75)\n",
    "y_train = one_hot(y_train_num, num_class=11)\n",
    "y_test = one_hot(y_test_num, num_class=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for hd in [10*x for x in range(10)]:\n",
    "    acc_list = []\n",
    "    for i in range(5):\n",
    "        net = NeuralNet(learning_rate=lr, hidden_dim=hd)\n",
    "        net.train(x_train, y_train_num, optim='sgd', verbose=False)\n",
    "        y_pred = net.predict(x_test)\n",
    "        test_acc = np.mean(y_pred == y_test_num)\n",
    "        acc_list.append(test_acc)\n",
    "        \n",
    "    avg = np.mean(acc_list)\n",
    "    results.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
